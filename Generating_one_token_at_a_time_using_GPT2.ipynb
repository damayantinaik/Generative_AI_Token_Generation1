{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damayantinaik/Generative_AI_Token_Generation1/blob/main/Generating_one_token_at_a_time_using_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6WYwb6N1dD7"
      },
      "source": [
        "# Generating one token at a time using GPT2 model\n",
        "\n",
        "In this project, I'll work how an GPT2 LLM generates text--one token at a time, using the previous tokens to predict the following ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HtKwDX01dD-"
      },
      "source": [
        "## Step 1. Load the tokenizer and model\n",
        "\n",
        "First I'll load a tokenizer and a model from HuggingFace's transformers library. A tokenizer is a function that splits a string into a list of numbers that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeZTrRgZ1dD_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pretrained model and a tokenizer using HuggingFace.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# We create a partial sentence and tokenize it.\n",
        "text = \"Udacity is the best place to learn about generative\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Let us show the tokens as numbers, i.e. \"input_ids\"\n",
        "inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFhiZ99M1dEB"
      },
      "source": [
        "## Step 2. Explore the tokenization output\n",
        "\n",
        "Let's explore what these tokens in more details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZxPl5KK1dEC"
      },
      "outputs": [],
      "source": [
        "# Let us show how the sentence is tokenized and put it in a Pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def show_tokenization(inputs):\n",
        "    return pd.DataFrame(\n",
        "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
        "        columns=[\"id\", \"token\"],\n",
        "    )\n",
        "\n",
        "\n",
        "show_tokenization(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWnDgS91dEC"
      },
      "source": [
        "### Subword tokenization\n",
        "\n",
        "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
        "\n",
        "## Step 2. Calculate the probability of the next token\n",
        "\n",
        "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM7H9wEq1dEC"
      },
      "outputs": [],
      "source": [
        "# Let us calculate the probabilities for the next token for all possible choices. I'll show the\n",
        "# top 5 choices and the corresponding words or subwords for these tokens.\n",
        "\n",
        "import torch\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "\n",
        "def show_next_token_choices(probabilities, top_n=5):\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            (id, tokenizer.decode(id), p.item())\n",
        "            for id, p in enumerate(probabilities)\n",
        "            if p.item()\n",
        "        ],\n",
        "        columns=[\"id\", \"token\", \"p\"],\n",
        "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
        "\n",
        "\n",
        "show_next_token_choices(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THq4keIy1dED"
      },
      "source": [
        "Interesting! The model thinks that the most likely next word is \"programming\", followed up closely by \"learning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xFTg7HU1dED"
      },
      "outputs": [],
      "source": [
        "# Obtain the token id for the most probable next token\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "\n",
        "print(f\"Next token id: {next_token_id}\")\n",
        "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqB9BEPN1dEE"
      },
      "outputs": [],
      "source": [
        "# We append the most likely token to the text.\n",
        "text = text + tokenizer.decode(next_token_id)\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDrtAavo1dEE"
      },
      "source": [
        "## Step 3. Generate some more tokens\n",
        "\n",
        "The following cell will take `text`, show the most probable tokens to follow, and append the most likely token to text. Run the cell over and over to see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT9Praz-1dEF"
      },
      "outputs": [],
      "source": [
        "# Let us Run this cell again and again to see how the text is generated.\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Show the text\n",
        "print(text)\n",
        "\n",
        "# Convert to tokens\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Let us calculate the probabilities for the next token and show the top 5 choices\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "display(Markdown(\"**Next token probabilities:**\"))\n",
        "display(show_next_token_choices(probabilities))\n",
        "\n",
        "# Let us take the most likely token id and add it to the text\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "text = text + tokenizer.decode(next_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ktvXffM1dEF"
      },
      "source": [
        "## Step 4. Use the `generate` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yejfwVYJ1dEF"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Start with some text and tokenize it\n",
        "text = \"Once upon a time, generative models\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Use the `generate` method to generate lots of text\n",
        "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Show the generated text\n",
        "display(Markdown(tokenizer.decode(output[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cygv4QuG1dEG"
      },
      "source": [
        "### That's interesting...\n",
        "\n",
        "Here, we notice that GPT-2 is not nearly as sophisticated as later models like GPT-4. It often repeats itself and doesn't always make much sense. But it's still pretty impressive that it can generate text that looks like English.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}